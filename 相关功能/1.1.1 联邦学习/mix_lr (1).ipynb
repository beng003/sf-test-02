{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix Federated Learning - Logistic Regression\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following codes are demos only. It's **NOT for production** due to system security concerns, please **DO NOT** use it directly in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to do `Logistic Regression` with mix partitioned data.\n",
    "\n",
    "The following is an example of mix partitioning data.\n",
    "\n",
    "<img alt=\"dataframe.png\" src=\"../developer/algorithm/resources/mix_data.png\" width=\"600\">\n",
    "\n",
    "Secretflow supports `Logistic Regression` with mix data. The algorithm combines `H`omomorphic `E`ncryption and secure aggregation for better security, for more details please refer to [Federated Logistic Regression](../developer/algorithm/mix_lr.md)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init secretflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of SecretFlow: 1.7.0b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beng003/anaconda/envs/sf_1.7/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-08-01 15:40:56,342\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-01 15:41:56,382 E 1361065 1361065] (raylet) node_manager.cc:3024: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448, IP: 10.0.0.4) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.0.4`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-01 15:42:56,383 E 1361065 1361065] (raylet) node_manager.cc:3024: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448, IP: 10.0.0.4) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.0.4`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-01 15:43:56,385 E 1361065 1361065] (raylet) node_manager.cc:3024: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448, IP: 10.0.0.4) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.0.4`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))\n",
    "\n",
    "# In case you have running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol', 'dave', 'eric'], address='local', num_cpus=64)\n",
    "\n",
    "alice, bob, carol, dave, eric = (\n",
    "    sf.PYU('alice'),\n",
    "    sf.PYU('bob'),\n",
    "    sf.PYU('carol'),\n",
    "    sf.PYU('dave'),\n",
    "    sf.PYU('eric'),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [brease canser](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)) as our dataset.\n",
    "\n",
    "Let us build a mix partitioned data with this dataset. The partitions are as follows:\n",
    "\n",
    "|label|feature1\\~feature10|feature11\\~feature20|feature21\\~feature30|\n",
    "|---|---|---|--|\n",
    "|alice_y0|alice_x0|bob_x0|carol_x|\n",
    "|alice_y1|alice_x1|bob_x1|dave_x|\n",
    "|alice_y2|alice_x2|bob_x2|eric_x|\n",
    "\n",
    "Alice holds all label and all 1~10 features, bob holds all 11~20 fetures,\n",
    "carol/dave/eric hold a part of 21~30 features separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features, label = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "features.iloc[:, :] = StandardScaler().fit_transform(features)\n",
    "label = label.to_frame()\n",
    "\n",
    "\n",
    "feat_list = [\n",
    "    features.iloc[:, :10],\n",
    "    features.iloc[:, 10:20],\n",
    "    features.iloc[:, 20:],\n",
    "]\n",
    "\n",
    "alice_y0, alice_y1, alice_y2 = label.iloc[0:1], label.iloc[1:2], label.iloc[2:3]\n",
    "alice_x0, alice_x1, alice_x2 = (\n",
    "    feat_list[0].iloc[0:1, :],\n",
    "    feat_list[0].iloc[1:2, :],\n",
    "    feat_list[0].iloc[2:3, :],\n",
    ")\n",
    "bob_x0, bob_x1, bob_x2 = (\n",
    "    feat_list[1].iloc[0:1, :],\n",
    "    feat_list[1].iloc[1:2, :],\n",
    "    feat_list[1].iloc[2:3, :],\n",
    ")\n",
    "carol_x, dave_x, eric_x = (\n",
    "    feat_list[2].iloc[0:1, :],\n",
    "    feat_list[2].iloc[1:2, :],\n",
    "    feat_list[2].iloc[2:3, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "\n",
    "def filepath(filename):\n",
    "    return f'{tmp_dir}/{filename}'\n",
    "\n",
    "\n",
    "alice_y0_file, alice_y1_file, alice_y2_file = (\n",
    "    filepath('alice_y0'),\n",
    "    filepath('alice_y1'),\n",
    "    filepath('alice_y2'),\n",
    ")\n",
    "alice_x0_file, alice_x1_file, alice_x2_file = (\n",
    "    filepath('alice_x0'),\n",
    "    filepath('alice_x1'),\n",
    "    filepath('alice_x2'),\n",
    ")\n",
    "bob_x0_file, bob_x1_file, bob_x2_file = (\n",
    "    filepath('bob_x0'),\n",
    "    filepath('bob_x1'),\n",
    "    filepath('bob_x2'),\n",
    ")\n",
    "carol_x_file, dave_x_file, eric_x_file = (\n",
    "    filepath('carol_x'),\n",
    "    filepath('dave_x'),\n",
    "    filepath('eric_x'),\n",
    ")\n",
    "\n",
    "alice_x0.to_csv(alice_x0_file, index=False)\n",
    "alice_x1.to_csv(alice_x1_file, index=False)\n",
    "alice_x2.to_csv(alice_x2_file, index=False)\n",
    "bob_x0.to_csv(bob_x0_file, index=False)\n",
    "bob_x1.to_csv(bob_x1_file, index=False)\n",
    "bob_x2.to_csv(bob_x2_file, index=False)\n",
    "carol_x.to_csv(carol_x_file, index=False)\n",
    "dave_x.to_csv(dave_x_file, index=False)\n",
    "eric_x.to_csv(eric_x_file, index=False)\n",
    "\n",
    "alice_y0.to_csv(alice_y0_file, index=False)\n",
    "alice_y1.to_csv(alice_y1_file, index=False)\n",
    "alice_y2.to_csv(alice_y2_file, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create `MixDataFrame` x and y for further usage.\n",
    "`MixDataFrame` is a list of `HDataFrame` or `VDataFrame`\n",
    "\n",
    "> you can read secretflow's [DataFrame](../user_guide/preprocessing/DataFrame.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party bob.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party carol.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party bob.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party dave.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party bob.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party eric.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n"
     ]
    }
   ],
   "source": [
    "vdf_x0 = sf.data.vertical.read_csv(\n",
    "    {alice: alice_x0_file, bob: bob_x0_file, carol: carol_x_file}\n",
    ")\n",
    "vdf_x1 = sf.data.vertical.read_csv(\n",
    "    {alice: alice_x1_file, bob: bob_x1_file, dave: dave_x_file}\n",
    ")\n",
    "vdf_x2 = sf.data.vertical.read_csv(\n",
    "    {alice: alice_x2_file, bob: bob_x2_file, eric: eric_x_file}\n",
    ")\n",
    "vdf_y0 = sf.data.vertical.read_csv({alice: alice_y0_file})\n",
    "vdf_y1 = sf.data.vertical.read_csv({alice: alice_y1_file})\n",
    "vdf_y2 = sf.data.vertical.read_csv({alice: alice_y2_file})\n",
    "\n",
    "\n",
    "x = sf.data.mix.MixDataFrame(partitions=[vdf_x0, vdf_x1, vdf_x2])\n",
    "y = sf.data.mix.MixDataFrame(partitions=[vdf_y0, vdf_y1, vdf_y2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct `HEU` and `SecureAggregator` for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m WARNING: 32 PYTHON worker processes have been started on node: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448 with address: 10.0.0.4. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 40 PYTHON worker processes have been started on node: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448 with address: 10.0.0.4. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Create proxy actor <class 'secretflow.security.aggregation.secure_aggregator._Masker'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.security.aggregation.secure_aggregator._Masker'> with party bob.\n",
      "INFO:root:Create proxy actor <class 'secretflow.security.aggregation.secure_aggregator._Masker'> with party carol.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Task was killed due to the node running low on memory.\nMemory on the node (IP: 10.0.0.4, ID: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448) where the task (task ID: ffffffffffffffff8c0353457ff6e45a59d8ba2e01000000, name=_Masker.__init__, pid=1362246, memory used=0.08GB) was running was 14.38GB / 15.11GB (0.951635), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b98c7cd8ed949909f445d43393bc3f7ce187c87a6ab72aa2b97a85eb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-b98c7cd8ed949909f445d43393bc3f7ce187c87a6ab72aa2b97a85eb*out -ip 10.0.0.4. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n1321436\t0.89\t/home/beng003/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/node...\n1322060\t0.70\t/home/beng003/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/node...\n1360907\t0.24\t/home/beng003/anaconda/envs/sf_1.7/bin/python -m ipykernel_launcher --f=/home/beng003/.local/share/j...\n1361957\t0.20\tray::HEUSkKeeper\n1362001\t0.20\tray::HEUSkKeeper\n1361999\t0.20\tray::HEUEvaluator\n1362000\t0.20\tray::HEUEvaluator\n1362125\t0.20\tray::HEUSkKeeper\n1362123\t0.20\tray::HEUEvaluator\n1362124\t0.20\tray::HEUEvaluator\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m heu1 \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mHEU(heu_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdave\u001b[39m\u001b[38;5;124m'\u001b[39m]), spu\u001b[38;5;241m.\u001b[39mspu_pb2\u001b[38;5;241m.\u001b[39mFM128)\n\u001b[1;32m     21\u001b[0m heu2 \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mHEU(heu_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meric\u001b[39m\u001b[38;5;124m'\u001b[39m]), spu\u001b[38;5;241m.\u001b[39mspu_pb2\u001b[38;5;241m.\u001b[39mFM128)\n\u001b[0;32m---> 22\u001b[0m aggregator0 \u001b[38;5;241m=\u001b[39m \u001b[43mSecureAggregator\u001b[49m\u001b[43m(\u001b[49m\u001b[43malice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43malice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m aggregator1 \u001b[38;5;241m=\u001b[39m SecureAggregator(alice, [alice, bob, dave])\n\u001b[1;32m     24\u001b[0m aggregator2 \u001b[38;5;241m=\u001b[39m SecureAggregator(alice, [alice, bob, eric])\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/secretflow/security/aggregation/secure_aggregator.py:183\u001b[0m, in \u001b[0;36mSecureAggregator.__init__\u001b[0;34m(self, device, participants, fxp_bits)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fxp_bits \u001b[38;5;241m=\u001b[39m fxp_bits\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maskers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    181\u001b[0m     pyu: _Masker(pyu\u001b[38;5;241m.\u001b[39mparty, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fxp_bits, device\u001b[38;5;241m=\u001b[39mpyu) \u001b[38;5;28;01mfor\u001b[39;00m pyu \u001b[38;5;129;01min\u001b[39;00m participants\n\u001b[1;32m    182\u001b[0m }\n\u001b[0;32m--> 183\u001b[0m pub_keys \u001b[38;5;241m=\u001b[39m \u001b[43mreveal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mpyu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparty\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpyu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maskers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m masker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maskers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    187\u001b[0m     masker\u001b[38;5;241m.\u001b[39mgen_rng(pub_keys)\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/secretflow/device/driver.py:162\u001b[0m, in \u001b[0;36mreveal\u001b[0;34m(func_or_object, heu_encoder)\u001b[0m\n\u001b[1;32m    159\u001b[0m         logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting teeu data from TEEU \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m cur_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 162\u001b[0m all_object \u001b[38;5;241m=\u001b[39m \u001b[43msfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_object_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m new_flatten_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flatten_val:\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/secretflow/distributed/primitive.py:158\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fed\u001b[38;5;241m.\u001b[39mget(object_refs)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_distribution_mode() \u001b[38;5;241m==\u001b[39m DISTRIBUTION_MODE\u001b[38;5;241m.\u001b[39mSIMULATION:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_distribution_mode() \u001b[38;5;241m==\u001b[39m DISTRIBUTION_MODE\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_refs\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:22\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/sf_1.7/lib/python3.10/site-packages/ray/_private/worker.py:2626\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2624\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2626\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   2629\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Task was killed due to the node running low on memory.\nMemory on the node (IP: 10.0.0.4, ID: b17a725100fa77fdcb0284852df0aed02ea5a7b7220eb47301a9d448) where the task (task ID: ffffffffffffffff8c0353457ff6e45a59d8ba2e01000000, name=_Masker.__init__, pid=1362246, memory used=0.08GB) was running was 14.38GB / 15.11GB (0.951635), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b98c7cd8ed949909f445d43393bc3f7ce187c87a6ab72aa2b97a85eb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-b98c7cd8ed949909f445d43393bc3f7ce187c87a6ab72aa2b97a85eb*out -ip 10.0.0.4. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n1321436\t0.89\t/home/beng003/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/node...\n1322060\t0.70\t/home/beng003/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/node...\n1360907\t0.24\t/home/beng003/anaconda/envs/sf_1.7/bin/python -m ipykernel_launcher --f=/home/beng003/.local/share/j...\n1361957\t0.20\tray::HEUSkKeeper\n1362001\t0.20\tray::HEUSkKeeper\n1361999\t0.20\tray::HEUEvaluator\n1362000\t0.20\tray::HEUEvaluator\n1362125\t0.20\tray::HEUSkKeeper\n1362123\t0.20\tray::HEUEvaluator\n1362124\t0.20\tray::HEUEvaluator\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero."
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from secretflow.security.aggregation import SecureAggregator\n",
    "import spu\n",
    "\n",
    "\n",
    "def heu_config(sk_keeper: str, evaluators: List[str]):\n",
    "    return {\n",
    "        'sk_keeper': {'party': sk_keeper},\n",
    "        'evaluators': [{'party': evaluator} for evaluator in evaluators],\n",
    "        'mode': 'PHEU',\n",
    "        'he_parameters': {\n",
    "            'schema': 'paillier',\n",
    "            'key_pair': {'generate': {'bit_size': 2048}},\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "heu0 = sf.HEU(heu_config('alice', ['bob', 'carol']), spu.spu_pb2.FM128)\n",
    "heu1 = sf.HEU(heu_config('alice', ['bob', 'dave']), spu.spu_pb2.FM128)\n",
    "heu2 = sf.HEU(heu_config('alice', ['bob', 'eric']), spu.spu_pb2.FM128)\n",
    "aggregator0 = SecureAggregator(alice, [alice, bob, carol])\n",
    "aggregator1 = SecureAggregator(alice, [alice, bob, dave])\n",
    "aggregator2 = SecureAggregator(alice, [alice, bob, eric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:MixLr epoch 0: loss = 0.22200048124132613\n",
      "INFO:root:MixLr epoch 1: loss = 0.10997288443236536\n",
      "INFO:root:MixLr epoch 2: loss = 0.08508413270494121\n",
      "INFO:root:MixLr epoch 3: loss = 0.07325763613227645\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "from secretflow.ml.linear import FlLogisticRegressionMix\n",
    "\n",
    "model = FlLogisticRegressionMix()\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=3,\n",
    "    learning_rate=0.1,\n",
    "    aggregators=[aggregator0, aggregator1, aggregator2],\n",
    "    heus=[heu0, heu1, heu2],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do predictions with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.9875535119708261 , acc: 0.9384885764499121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = np.concatenate(sf.reveal(model.predict(x)))\n",
    "\n",
    "auc = roc_auc_score(label.values, y_pred)\n",
    "acc = np.mean((y_pred > 0.5) == label.values)\n",
    "print('auc:', auc, ', acc:', acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "Clean temp files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(tmp_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d1547304beaba725027c44e57cc46fc747862fe9496520910412a3187eb35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
