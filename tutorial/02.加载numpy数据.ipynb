{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b399a34",
   "metadata": {},
   "source": [
    "# Load Numpy data in SecretFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8eb49",
   "metadata": {},
   "source": [
    "The following codes are demos only. It's **NOT for production** due to system security concerns, please **DO NOT** use it directly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd8384",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to load Numpy data in a multi-party secure environment using SecretFlow.  \n",
    "SecretFlow supports multiple formats, including `.npy` and `.npz`, and its interface is designed to be compatible with `numpy` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383ac24",
   "metadata": {},
   "source": [
    "## Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ee31441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:32:53.579076Z",
     "start_time": "2024-07-10T08:32:53.527983Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "bb86c839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:33:00.710796Z",
     "start_time": "2024-07-10T08:32:55.298697Z"
    }
   },
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "sf.init(['alice', 'bob', 'charlie'], address=\"local\", log_to_driver=True)\n",
    "alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of SecretFlow: 1.7.0.dev$$DATE$$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 08:32:59,716\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "7e4f9827",
   "metadata": {},
   "source": [
    "## Interface Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf743273",
   "metadata": {},
   "source": [
    "In SecretFlow, we provide an interface similar to `numpy.load` called `secretflow.load.ndarray.load` to load `ndarray` data from multiple parties and convert it into a federated representation. \n",
    "\n",
    " Using secretflow.data.load, you can read numpy files from multiple parties and create a `FedNdarray` object.\n",
    "\n",
    "Interface Introduction：[secretflow.data.load](https://www.secretflow.org.cn/docs/secretflow/en/source/secretflow.data.html#secretflow.data.ndarray.load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c30c9",
   "metadata": {},
   "source": [
    "## Data Download and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b53809f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:33:10.434633Z",
     "start_time": "2024-07-10T08:33:06.072439Z"
    }
   },
   "source": [
    "%%capture\n",
    "%%!\n",
    "wget https://secretflow-data.oss-accelerate.aliyuncs.com/datasets/mnist/mnist.npz\n",
    "pip install opencv-python"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ".npz 文件\n",
    ".npz 文件是 numpy 用来保存多个数组的文件格式。它是一个压缩的存档文件，包含一个或多个 .npy 格式的数组文件。每个数组都可以通过一个键来访问。"
   ],
   "id": "7b7ae92e07a48ac9"
  },
  {
   "cell_type": "code",
   "id": "fb1ca464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:35:02.368954Z",
     "start_time": "2024-07-10T08:35:02.333293Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "all_data = np.load(\"./mnist.npz\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:37:53.878430Z",
     "start_time": "2024-07-10T08:37:53.842823Z"
    }
   },
   "cell_type": "code",
   "source": "print(all_data.files)",
   "id": "d5adb9cbc1d47123",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'y_train', 'y_test']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "20ccb301",
   "metadata": {},
   "source": [
    "Splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "0ff318ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:38:23.249642Z",
     "start_time": "2024-07-10T08:38:22.564193Z"
    }
   },
   "source": [
    "alice_train_x = all_data[\"x_train\"][:30000]\n",
    "alice_test_x = all_data[\"x_test\"][:30000]\n",
    "alice_train_y = all_data[\"y_train\"][:30000]\n",
    "alice_test_y = all_data[\"y_test\"][:30000]\n",
    "\n",
    "bob_train_x = all_data[\"x_train\"][30000:]\n",
    "bob_test_x = all_data[\"x_test\"][30000:]\n",
    "bob_train_y = all_data[\"y_train\"][30000:]\n",
    "bob_test_y = all_data[\"y_test\"][30000:]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:38:44.520887Z",
     "start_time": "2024-07-10T08:38:44.482318Z"
    }
   },
   "cell_type": "code",
   "source": "alice_train_x",
   "id": "e3cd706df78e9f5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "41652edb",
   "metadata": {},
   "source": [
    "Saving separately as npz format file."
   ]
  },
  {
   "cell_type": "code",
   "id": "a8ece9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:39:32.565291Z",
     "start_time": "2024-07-10T08:39:32.258332Z"
    }
   },
   "source": [
    "np.savez(\n",
    "    \"./alice_mnist.npz\",\n",
    "    train_x=alice_train_x,\n",
    "    test_x=alice_test_x,\n",
    "    train_y=alice_train_y,\n",
    "    test_y=alice_test_y,\n",
    ")\n",
    "np.savez(\n",
    "    \"./bob_mnist.npz\",\n",
    "    train_x=bob_train_x,\n",
    "    test_x=bob_test_x,\n",
    "    train_y=bob_train_y,\n",
    "    test_y=bob_test_y,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "1a4ab72d",
   "metadata": {},
   "source": [
    "Saving tarin_x from Alice and Bob as npy format for convenient future reading."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d29231e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:39:59.495362Z",
     "start_time": "2024-07-10T08:39:59.412887Z"
    }
   },
   "source": [
    "np.save(\"./alice_mnist_train_x.npy\", alice_train_x)\n",
    "np.save(\"./bob_mnist_train_x.npy\", bob_train_x)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "bd135e73",
   "metadata": {},
   "source": [
    "##  Loading npz files"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e8e7578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:40:08.838042Z",
     "start_time": "2024-07-10T08:40:08.800487Z"
    }
   },
   "source": [
    "alice_path = \"./alice_mnist.npz\"\n",
    "bob_path = \"./bob_mnist.npz\""
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "67fc91ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:40:10.987986Z",
     "start_time": "2024-07-10T08:40:10.915432Z"
    }
   },
   "source": [
    "from secretflow.data.ndarray import load\n",
    "from secretflow.data.split import train_test_split"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "bcca6eff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:40:26.712183Z",
     "start_time": "2024-07-10T08:40:24.461020Z"
    }
   },
   "source": [
    "fed_npz = load({alice: alice_path, bob: bob_path}, allow_pickle=True)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "a66aa798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:40:31.864390Z",
     "start_time": "2024-07-10T08:40:31.822730Z"
    }
   },
   "source": [
    "fed_npz"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_x': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff8160>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff8580>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>),\n",
       " 'test_x': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff8640>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff89a0>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>),\n",
       " 'train_y': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff8af0>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff8dc0>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>),\n",
       " 'test_y': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff9270>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f1155ff9780>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "8cde8331",
   "metadata": {},
   "source": [
    "In FedNpz, each value represents a FedNdarray."
   ]
  },
  {
   "cell_type": "code",
   "id": "f9e378ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:40:35.101230Z",
     "start_time": "2024-07-10T08:40:35.060862Z"
    }
   },
   "source": [
    "type(fed_npz[\"train_x\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "secretflow.data.ndarray.ndarray.FedNdarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "5e044e91",
   "metadata": {},
   "source": [
    "## Loading npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a2c68",
   "metadata": {},
   "source": [
    "Loading npy is very simple. Directly call the load interface, and the results will be a standard FedNdarray object."
   ]
  },
  {
   "cell_type": "code",
   "id": "c718b1c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:41:47.709036Z",
     "start_time": "2024-07-10T08:41:47.667430Z"
    }
   },
   "source": [
    "alice_path = \"./alice_mnist_train_x.npy\"\n",
    "bob_path = \"./bob_mnist_train_x.npy\""
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "1fb4fe11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:41:59.101296Z",
     "start_time": "2024-07-10T08:41:58.973473Z"
    }
   },
   "source": [
    "fed_ndarray = load({alice: alice_path, bob: bob_path}, allow_pickle=True)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "0f21b86c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:42:41.345712Z",
     "start_time": "2024-07-10T08:42:41.306995Z"
    }
   },
   "source": [
    "type(fed_ndarray)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "secretflow.data.ndarray.ndarray.FedNdarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:42:52.372608Z",
     "start_time": "2024-07-10T08:42:52.332133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个包含 Python 对象的数组\n",
    "data = np.array([{'a': 1, 'b': 2}, {'a': 3, 'b': 4}], dtype=object)\n",
    "\n",
    "# 保存数组到 .npy 文件\n",
    "np.save('data.npy', data)\n",
    "\n",
    "# 尝试加载数据\n",
    "loaded_data = np.load('data.npy', allow_pickle=True)\n",
    "print(loaded_data)"
   ],
   "id": "975392e019ce2bcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1, 'b': 2} {'a': 3, 'b': 4}]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "936ba7ff",
   "metadata": {},
   "source": [
    "##  How can I convert my existing data into a FedNdarray and read it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fd75c",
   "metadata": {},
   "source": [
    "How can we convert other types of data into FedNdarray data?  \n",
    "If we have an image dataset or a speech dataset, how can we pass the data into a federated model using FedNdarray?  \n",
    "Let's take the flower classification dataset Flower as an example."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4158ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:43:44.143905Z",
     "start_time": "2024-07-10T08:43:29.778873Z"
    }
   },
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "_temp_dir = tempfile.mkdtemp()\n",
    "path_to_flower_dataset = tf.keras.utils.get_file(\n",
    "    \"flower_photos\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
    "    untar=True,\n",
    "    cache_dir=_temp_dir,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 08:43:31.342303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:53:12.863389Z",
     "start_time": "2024-07-10T08:53:12.800592Z"
    }
   },
   "cell_type": "code",
   "source": "path_to_flower_dataset",
   "id": "d7e9a1d74ce72a4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpw6jmg19l/datasets/flower_photos'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "f2e9c468",
   "metadata": {},
   "source": [
    "After downloading and extracting the dataset, the root directory of the dataset is \"flower_photos\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "e11441a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:54:13.863806Z",
     "start_time": "2024-07-07T12:54:04.168198Z"
    }
   },
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import cv2  # The dependencies need to be installed manually, pip install opencv-python\n",
    "\n",
    "root = path_to_flower_dataset\n",
    "classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "img_paths = []  # Used to save all picture paths\n",
    "labels = []  # Used to save the picture category tags,(0,1,2,3,4)\n",
    "for i, label in enumerate(classes):\n",
    "    cls_img_paths = glob.glob(os.path.join(root, label, \"*.jpg\"))\n",
    "    img_paths.extend(cls_img_paths)\n",
    "    labels.extend([i] * len(cls_img_paths))\n",
    "\n",
    "# image->numpy\n",
    "img_numpys = []\n",
    "labels = np.array(labels)\n",
    "for img_path in img_paths:\n",
    "    img_numpy = cv2.imread(img_path)\n",
    "    img_numpy = cv2.resize(img_numpy, (240, 240))\n",
    "    img_numpy = np.reshape(img_numpy, (1, 240, 240, 3))\n",
    "    # If use Pytorch backend dimension should be exchanged\n",
    "    # img_numpy = np.transpose(img_numpy, (0,3,1,2))\n",
    "    img_numpys.append(img_numpy)\n",
    "\n",
    "images = np.concatenate(img_numpys, axis=0)\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Distribute images and labels to two nodes, allocating 50% of the data to each node.\n",
    "per = 0.5\n",
    "alice_images = images[: int(per * images.shape[0]), :, :, :]\n",
    "alice_label = labels[: int(per * images.shape[0])]\n",
    "bob_images = images[int(per * images.shape[0]) :, :, :, :]\n",
    "bob_label = labels[int(per * images.shape[0]) :]\n",
    "print(\n",
    "    f\"alice images shape = {alice_images.shape}, alice labels shape = {alice_label.shape}\"\n",
    ")\n",
    "print(f\"bob images shape = {bob_images.shape}, bob labels shape = {bob_label.shape}\")\n",
    "\n",
    "# Save the data as npz files separately, and then send them to the two machines.\n",
    "np.savez(\"flower_alice.npz\", image=alice_images, label=alice_label)\n",
    "np.savez(\"flower_bob.npz\", image=bob_images, label=bob_label)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3670, 240, 240, 3)\n",
      "(3670,)\n",
      "alice images shape = (1835, 240, 240, 3), alice labels shape = (1835,)\n",
      "bob images shape = (1835, 240, 240, 3), bob labels shape = (1835,)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "942c44d6",
   "metadata": {},
   "source": [
    " Once you have obtained the required NPZ files, use the previously mentioned load function to read them into FedNdarray format. Then, input them into the model to begin training."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb97921c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:54:18.724748Z",
     "start_time": "2024-07-07T12:54:17.123218Z"
    }
   },
   "source": [
    "fed_flower_npz = load(\n",
    "    {alice: \"./flower_alice.npz\", bob: \"./flower_bob.npz\"}, allow_pickle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "b851ee52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:37.611554Z",
     "start_time": "2024-07-07T12:56:37.552537Z"
    }
   },
   "source": [
    "fed_flower_npz"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f3a10ae1870>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f3a10ae2ce0>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>),\n",
       " 'label': FedNdarray(partitions={PYURuntime(alice): <secretflow.device.device.pyu.PYUObject object at 0x7f3a10ae2fe0>, PYURuntime(bob): <secretflow.device.device.pyu.PYUObject object at 0x7f3a10ae33a0>}, partition_way=<PartitionWay.VERTICAL: 'vertical'>)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "0f76e617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:39.994759Z",
     "start_time": "2024-07-07T12:56:39.930243Z"
    }
   },
   "source": [
    "fed_image = fed_flower_npz[\"image\"]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "368fa687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:42.568078Z",
     "start_time": "2024-07-07T12:56:42.428805Z"
    }
   },
   "source": [
    "fed_image.partition_shape()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PYURuntime(alice): (1835, 240, 240, 3), PYURuntime(bob): (1835, 240, 240, 3)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d17ec85c",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2dd4a8",
   "metadata": {},
   "source": [
    "It is recommended to test the data after converting it to the ndarray type using a single-machine training engine to verify if the data format matches the model correctly. Then, you can proceed to test it using the SecretFlow federated framework, which can improve the efficiency of troubleshooting.  \n",
    "*Note: When using image datasets, it is important to pay attention to the dimension ordering.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
